#!/usr/bin/env python3
"""
Scaffold integration test files for a Spring AI example
Usage: python3 scripts/scaffold_integration_test.py <module-path> [--complexity simple|complex|mcp]
"""

import sys
import pathlib
import json
import argparse
import time
from typing import Dict, List

def get_module_info(module_path: str) -> Dict:
    """Extract information about the module"""
    module_dir = pathlib.Path(module_path)
    module_name = module_dir.name
    
    # Convert kebab-case to PascalCase for Java class name
    class_name = ''.join(word.capitalize() for word in module_name.replace('-', ' ').replace('_', ' ').split())
    
    # Try to determine if it's a Spring Boot app
    pom_path = module_dir / "pom.xml"
    has_spring_boot = False
    if pom_path.exists():
        pom_content = pom_path.read_text()
        has_spring_boot = "spring-boot" in pom_content.lower()
    
    return {
        'module_name': module_name,
        'class_name': class_name,
        'has_spring_boot': has_spring_boot,
        'module_dir': module_dir
    }

def get_config_template(complexity: str) -> Dict:
    """Get configuration template based on complexity"""
    templates = {
        'simple': {
            "timeoutSec": 120,
            "successRegex": ["BUILD SUCCESS", "Started.*Application"],
            "requiredEnv": ["OPENAI_API_KEY"],
            "aiValidation": {
                "enabled": True,
                "validationMode": "hybrid",
                "expectedBehavior": "Spring Boot application should start successfully and demonstrate basic functionality",
                "promptTemplate": "example_validation"
            }
        },
        'complex': {
            "timeoutSec": 300,  
            "successRegex": ["BUILD SUCCESS", "Started.*Application", "EVALUATION:\\\\s+PASS"],
            "requiredEnv": ["OPENAI_API_KEY"],
            "aiValidation": {
                "enabled": True,
                "validationMode": "primary",
                "readmeFile": "../README.md",
                "expectedBehavior": "Complex AI workflow should execute successfully with multi-step processing and produce evaluation results",
                "promptTemplate": "workflow_validation",
                "successCriteria": {
                    "expectedSteps": 3,
                    "evaluationRequired": True
                }
            }
        },
        'mcp': {
            "timeoutSec": 300,
            "successRegex": ["BUILD SUCCESS", "Started.*Application", "MCP Initialized", "Connected"],
            "requiredEnv": ["OPENAI_API_KEY"],
            "setupCommands": ["./create-database.sh"],
            "aiValidation": {
                "enabled": True,
                "validationMode": "hybrid",
                "readmeFile": "../README.md",
                "expectedBehavior": "MCP server/client should establish connections, register tools, and demonstrate protocol communication",
                "promptTemplate": "client_server_validation",
                "components": ["server", "client"],
                "successCriteria": {
                    "requiresUserInteraction": False,
                    "expectedOutputTypes": ["tool_discovery", "protocol_communication"]
                }
            }
        }
    }
    return templates.get(complexity, templates['simple'])

def generate_jbang_launcher(module_info: Dict, config: Dict) -> str:
    """Generate JBang launcher script using centralized utilities"""
    class_name = module_info['class_name']
    module_name = module_info['module_name']
    module_dir = module_info['module_dir']
    
    # Calculate the relative path to integration-testing/jbang-lib/
    # Count how many levels deep we are from the repository root
    path_parts = module_dir.parts
    depth = len(path_parts)
    
    # Adjust for the fact that we're writing to integration-tests subdirectory
    relative_path = "../" * (depth + 1) + "integration-testing/jbang-lib/"
    
    return f'''///usr/bin/env jbang "$0" "$@" ; exit $?
//DEPS org.zeroturnaround:zt-exec:1.12
//DEPS com.fasterxml.jackson.core:jackson-databind:2.17.1
//JAVA 17
//FILES ExampleInfo.json
//SOURCES {relative_path}IntegrationTestUtils.java

/*
 * Integration test launcher for {module_name}
 * Uses centralized utilities from IntegrationTestUtils
 * Generated by scaffold_integration_test.py on {time.strftime('%Y-%m-%d %H:%M:%S')}
 */

public class Run{class_name} {{
    
    public static void main(String... args) throws Exception {{
        IntegrationTestUtils.runIntegrationTest("{module_name}");
    }}
}}'''

def create_integration_test(module_path: str, complexity: str = 'simple', force: bool = False, ai_validation: bool = True, ai_mode: str = None):
    """Create integration test files for a Spring AI example"""
    module_dir = pathlib.Path(module_path)
    if not module_dir.exists():
        print(f"‚ùå Module directory not found: {module_path}")
        sys.exit(1)
    
    if not (module_dir / "pom.xml").exists():
        print(f"‚ùå No pom.xml found in {module_path}. Is this a Maven project?")
        sys.exit(1)
    
    integration_dir = module_dir / "integration-tests"
    
    # Check if integration tests already exist
    if integration_dir.exists() and not force:
        print(f"‚ö†Ô∏è  Integration tests already exist in {integration_dir}")
        print("Use --force to overwrite existing files")
        sys.exit(1)
    
    integration_dir.mkdir(exist_ok=True)
    
    # Get module information
    module_info = get_module_info(module_path)
    config = get_config_template(complexity)
    
    # Apply AI validation options
    if not ai_validation:
        # Remove AI validation section
        config.pop('aiValidation', None)
    elif ai_mode:
        # Override validation mode if specified
        if 'aiValidation' in config:
            config['aiValidation']['validationMode'] = ai_mode
    
    # Create ExampleInfo.json
    config_file = integration_dir / "ExampleInfo.json"
    with config_file.open("w") as f:
        json.dump(config, f, indent=2)
    
    # Create JBang launcher
    launcher_file = integration_dir / f"Run{module_info['class_name']}.java"
    launcher_content = generate_jbang_launcher(module_info, config)
    with launcher_file.open("w") as f:
        f.write(launcher_content)
    
    ai_enabled = 'aiValidation' in config
    ai_mode = config.get('aiValidation', {}).get('validationMode', 'N/A') if ai_enabled else 'disabled'
    
    print(f"‚úÖ Created integration test for {module_path}")
    print(f"   üìÑ {config_file}")
    print(f"   ‚òï {launcher_file} (using centralized utilities)")
    print(f"   üéØ Complexity: {complexity}")
    print(f"   ‚è±Ô∏è  Timeout: {config['timeoutSec']}s")
    print(f"   üîç Success patterns: {len(config['successRegex'])}")
    print(f"   ü§ñ AI validation: {ai_mode}")
    print(f"\nüß™ To test locally:")
    print(f"   cd {module_path}")
    print(f"   jbang integration-tests/Run{module_info['class_name']}.java")
    
    # Show configuration suggestions
    print(f"\nüí° Configuration suggestions:")
    print("   - The JBang script uses centralized utilities (no code duplication)")
    print("   - All configuration is done through ExampleInfo.json")
    
    if ai_enabled:
        print(f"   - AI validation enabled ({ai_mode} mode) for intelligent validation")
        print("   - Review 'expectedBehavior' in aiValidation section")
        print("   - Choose appropriate promptTemplate (example_validation, workflow_validation, client_server_validation)")
        if ai_mode == 'primary':
            print("   - Primary mode: Only AI validation (good for unpredictable AI outputs)")
        elif ai_mode == 'hybrid':
            print("   - Hybrid mode: Both regex and AI validation must pass")
        else:
            print("   - Fallback mode: AI validation if regex fails")
    else:
        print("   - AI validation disabled - using regex patterns only")
        print("   - Consider enabling AI validation for complex AI workflows")
    
    if complexity == 'simple':
        print("   - Review success patterns in ExampleInfo.json")
        print("   - Adjust timeout if needed")
    elif complexity == 'mcp':
        print("   - Verify setupCommands match your database/service setup")
        print("   - Add cleanup commands if needed")
        print("   - Check MCP-specific environment variables")
        print("   - AI validation excellent for MCP protocol validation")
    else:
        print("   - Fine-tune success patterns for your specific workflow")
        print("   - Consider adding setup/cleanup commands if needed")
        print("   - AI validation ideal for complex workflows with unpredictable outputs")

def main():
    """Main CLI function"""
    parser = argparse.ArgumentParser(
        description="Scaffold integration test files for Spring AI examples",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Create simple integration test with AI validation (default)
  python3 scripts/scaffold_integration_test.py kotlin/kotlin-hello-world
  
  # Create complex integration test with primary AI validation
  python3 scripts/scaffold_integration_test.py agentic-patterns/chain-workflow --complexity complex --ai-mode primary
  
  # Create MCP integration test with hybrid validation
  python3 scripts/scaffold_integration_test.py model-context-protocol/sqlite/simple --complexity mcp
  
  # Create integration test without AI validation (regex only)
  python3 scripts/scaffold_integration_test.py misc/simple-example --no-ai-validation
  
  # Override existing files with fallback AI validation
  python3 scripts/scaffold_integration_test.py existing/module --force --ai-mode fallback
        """
    )
    
    parser.add_argument("module_path", 
                       help="Path to the module directory (e.g., kotlin/kotlin-hello-world)")
    parser.add_argument("--complexity", "-c", 
                       choices=['simple', 'complex', 'mcp'], 
                       default='simple',
                       help="Complexity level for the integration test")
    parser.add_argument("--force", "-f", 
                       action="store_true",
                       help="Overwrite existing integration test files")
    parser.add_argument("--no-ai-validation", 
                       action="store_true",
                       help="Disable AI validation (use only regex patterns)")
    parser.add_argument("--ai-mode", 
                       choices=['primary', 'hybrid', 'fallback'],
                       help="Override AI validation mode (primary=AI only, hybrid=AI+regex, fallback=regex first)")
    
    args = parser.parse_args()
    
    create_integration_test(args.module_path, args.complexity, args.force, 
                          not args.no_ai_validation, args.ai_mode)

if __name__ == "__main__":
    main()